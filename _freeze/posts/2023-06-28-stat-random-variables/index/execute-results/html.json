{
  "hash": "13d88d0ded6cb058e3afb5a14b576612",
  "result": {
    "markdown": "---\ntitle: \"Univariate Random Variables and its Properties\"\ndate: \"2023-06-28\"\ndescription: Explores the characteristics and properties of single-variable random distributions.\ncategories: [statistics]\nformat:\n  html: \n    code-fold: false\n---\n\n\n## Random Variables\n\n$\\textbf{Definition 1:}$ A random variable is a function $X$ which assigns to each element $s \\in S$ one and only one number $X(s) = x$, where $x \\in S_{x} \\subseteq \\mathbb{R}$, i.e.,\n$$\nX: S \\longrightarrow S_{x} \\subseteq \\mathbb{R}\n$$\nRandom variables help us to view the collected data as an *outcome* of a random experiment, which is a procedure whose outcome is unknown to the experimenter before it takes place.\n\nIn practice, we often use $\\Omega_{X}$ to denote support, or say sample sample, of random variable $X$. However, there are numerous notations to denote a support, e.g., $\\Omega, S, S_{X}, \\operatorname{supp}(X)$.\n\nTake throwing coins for example, if we only throw the coin for one time, then the sample space would be $\\{H, T\\}$, where $H$ represents head and $T$ represents tail. If we throw the coin for two times, now the sample space would be $\\{HH, HT, TH, TT\\}$.\n\nAnother example is tossing two dice. We toss two dice. In this case, the sample space is\n$$\n\\begin{aligned}\nS = \\{(1, 1), (1, 2), (1, 3), (1, 4), (1, 5), (1, 6),\\\\\n(2, 1), (2, 2), (2, 3), (2, 4), (2, 5), (2, 6),\\\\\n(3, 1), (3, 2), (3, 3), (3, 4), (3, 5), (3, 6),\\\\\n(4, 1), (4, 2), (4, 3), (4, 4), (4, 5), (4, 6),\\\\\n(5, 1), (5, 2), (5, 3), (5, 4), (5, 5), (5, 6),\\\\\n(6, 1), (6, 2), (6, 3), (6, 4), (6, 5), (6, 6)\\}\n\\end{aligned}\n$$\n\nOne may define the random variable X as the sum:\n$$\nS_{X} = \\{2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12\\}.\n$$\nor let the random variable $Y$ be the product:\n$$\nS_{Y} = \\{1, 2, 3, 4, 5, 6, 8, 9, 10, 12, 15, 16, 18, 20, 24, 25, 30, 36\\}.\n$$\n\nWe can use Monte Carlo experiment to simlulate the process and result of tossing a dice.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# d for dice\nd <- sample(1:6, size = 100, replace = TRUE)\nd\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  [1] 4 4 4 2 5 3 1 1 1 5 5 4 2 3 2 4 5 1 1 2 4 4 1 5 3 4 4 6 2 1 1 6 5 4 5 1 1\n [38] 5 5 1 6 6 1 5 2 3 4 6 1 4 2 1 3 6 6 3 2 2 5 2 2 5 3 6 4 3 1 4 4 4 1 3 1 2\n [75] 1 4 4 5 6 5 6 4 4 2 2 4 4 6 1 6 4 5 3 5 3 2 3 2 2 5\n```\n:::\n:::\n\n\nUsing `table` function we can see the probabilty of each outcome when tossing a dice.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntable(d) / 100\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nd\n   1    2    3    4    5    6 \n0.19 0.17 0.12 0.23 0.17 0.12 \n```\n:::\n\n```{.r .cell-code}\nplot(table(d) / 100,\n     xlab = \"Outcome of Dice\",\n     ylab = \"Probability of the Outcome\")\n\n# Add a title\ntitle(\"Probability Mass Function (PMF)\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nIf we toss it for 1000000 times, then we will find that the probability of each outcome would be very close to $1/6 \\approx 0.166667$, which yields a conclusion that given a larger the sample size, the closer the relative frequencies to the probabilities would be.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd <- sample(1:6, size = 1000000, replace = TRUE)\ntable(d) / 1000000\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nd\n       1        2        3        4        5        6 \n0.166437 0.166520 0.166535 0.166833 0.166419 0.167256 \n```\n:::\n:::\n\n\nNote that we can use the support of to categorize random variable. For a discrete random variable, its support must be a discrete set, while for a continuous random variable, its support is an interval of values.\n\n\n### Discrete Random Variables\n\n$\\textbf{Definition 2:}$ If a random variable $X$ has a discrete distribution with $\\operatorname{supp}(X) \\in \\mathbb{R}$, and define a function $f(x): \\mathbb{R} \\mapsto [0,1]$ to be\n$$\nf(x) = \n\\begin{cases}\n  \\mathbb{P}(X = x),& \\quad x \\in \\operatorname{supp}(X)\\\\\n  0,& \\quad x \\notin \\operatorname{supp}(X)\n\\end{cases}\n$$\nwhich satisfies:\n\n- $f(x) > 0, \\forall x \\in \\operatorname{supp}(X)$\n- $\\sum_{x \\in \\operatorname{supp}(X)} f(x) = 1$ \n- $\\mathbb{P}(X \\in A) = \\sum_{x \\in A}f(x)$, where $A \\in \\operatorname{supp}(X)$\n\nthen we say $f(x)$ is a *probability mass function* (PMF).\n\n$\\textbf{Definition 3:}$ We say a function $F(x)$ is a *cumulative distribution function* (CDF) if for any $x \\in \\mathbb{R}$, $F(x): \\mathbb{R} \\mapsto [0, 1]$, which satisfies\n$$\nF(x) = \\mathbb{P}(X \\leq x) = \\sum_{x_{i} \\in x}f(x)\n$$\nHere are some properties of discrete cumulative distribution function:\n\n1. $\\mathbb{P}(X > a) = 1 - \\mathbb{P}(X \\leq a) = 1 - F(a)$\n2. $\\mathbb{P}(X \\geq a) = \\mathbb{P}(X = a) + \\mathbb{P}(X > a) = f(a) + 1 - F(a) = [1 - F(a)] + f(a)$\n3. $\\mathbb{P}(a < X \\leq b) = F(b) - F(a)$\n4. $\\mathbb{P}(a < X < b) = F(b) - F(a) - f(b)$\n5. $\\mathbb{P}(a \\leq X \\leq b) = F(b) - F(a) + f(a)$\n6. $\\mathbb{P}(a \\leq X < b) = F(b) - F(a) + f(a) - f(b)$\n7. $\\lim_{x \\to -\\infty}F(x) = 0$, and $\\lim_{x \\to \\infty}F(x) = 1$\n\n### Continuous Random Variables\n\n$\\textbf{Definition 4:}$ For a continuous random variable $X$, if $f(x): \\mathbb{R} \\mapsto \\mathbb{R}$ and for any real number $a \\leq b$ satisfy\n$$\n\\mathbb{P}(a < X \\leq b) = \\int^{b}_{a}f(x) dx = F(x)\\Big|^{b}_{a} = F(b) - F(a)\n$$\nand\n\n- $f(x) > 0, \\forall \\operatorname{supp}(X) \\in \\Omega_{X}$\n- $\\int_{x \\in \\Omega_{X}}f(x)dx = 1$\n\nthen we say $f(x)$ is a *probability density function* (PDF) of $X$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Generate x values for the range of interest\nx <- seq(-4, 4, 0.01)\n\n# Compute corresponding y values using the standard normal density function\ny <- dnorm(x, mean = 0, sd = 1)\n\n# Create a blank plot with the desired x-axis limits and y-axis labels\nplot(x, y, type = \"l\", xlab = \"x\", ylab = \"Density\", xlim = c(-4, 4), ylim = c(0, 0.45), axes = F)\n\n# Fill the region between -1 and 1 with gray color\npolygon(c(-1, x[x >= -1 & x <= 1], 1), c(0, y[x >= -1 & x <= 1], 0), col = \"gray\")\n\naxis(1)\naxis(2)\n\n# Add a title\ntitle(\"Probability Density Function (PDF)\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\nNote that there is a corr\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Generate x values for the range of interest\nx <- seq(-4, 4, 0.01)\n\n# Compute corresponding y values for PDF and CDF\nPDF <- dnorm(x, mean = 0, sd = 1)\nCDF <- pnorm(x, mean = 0, sd = 1)\n\n# Set up a layout with two plots side by side\nlayout(matrix(c(1, 2), nrow = 1, ncol = 2), widths = c(0.5, 0.5))\n\n# Plot PDF on the left\npar(mar = c(5, 4, 4, 2))\nplot(x, PDF, type = \"l\", xlab = \"x\", ylab = \"Density\", main = \"PDF\")\nabline(v = c(-1, 1), lty = 2, col = \"gray\")  # Add vertical lines at -1 and 1\n\n# Plot CDF on the right\npar(mar = c(5, 2, 4, 4))\nplot(x, CDF, type = \"l\", xlab = \"x\", ylab = \"Probability\", main = \"CDF\")\nabline(v = c(-1, 1), lty = 2, col = \"gray\")  # Add vertical lines at -1 and 1\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){fig-align='center' width=672}\n:::\n:::\n\nMathematically, by using the fundamental theorem of calculus, we can find the relationship. Given a well-defined random variable $X$. Write its CDF as $F(x)$ and its PDF as $f(x)$, Since\n$$\n\\int^{b}_{a} f(x) dx = F(x) \\Big|^{b}_{a}\n$$\nthen\n$$\nf(x) = \\frac{\\partial F(x)}{\\partial x}, \\quad F(x) = \\int^{x}_{-\\infty}f(u)du.\n$$\nNow we will discuss some properties of CDF given continuous random variable. Suppose $X$ is a continuous random variable, and $\\operatorname{supp}(X) = \\{x| -\\infty < x < \\infty\\} = \\{x | x \\in \\mathbb{R}\\}$. Then\n\n1. $F(x) = \\mathbb{P}(X \\leq x) = \\int^{x}_{-\\infty}f(u)du = \\mathbb{P}(X < x)$\n2. If $a < b$, then $F(a) \\leq F(b)$\n3. $\\mathbb{P}(a < X < b) = \\mathbb{P}(a \\leq X \\leq b) = F(b) - F(a)$\n4. $\\lim_{x \\to -\\infty}F(x) = 0$, and $\\lim_{x \\to \\infty}F(x) = 1$\n5. $\\mathbb{P}(X \\geq x) = \\mathbb{P}(X > x) = 1- \\mathbb{P}(X \\leq x) = 1- F(x)$\n\n## Moments of Random Variables\n\nIn this section, we will talk about moment of random variables. Learning about moments of random variables is valuable for two main reasons:\n\n1. Characterizing Distributions: Moments provide a way to quantitatively describe the properties of probability distributions. They capture essential features such as the central tendency, variability, skewness, and kurtosis of a random variable's distribution. Understanding moments allows us to compare and contrast different distributions and gain insights into their shapes and characteristics.\n\n2. Statistical Analysis: Moments play a crucial role in statistical analysis. They provide important summary statistics that can be used for inference, hypothesis testing, and parameter estimation. Moments can help estimate population parameters, assess the goodness of fit, and determine the stability and convergence of statistical models.\n\n### Important Moments in Statistical Analysis\n\n$\\textbf{Theorem 1:}$ Let $X$ be a random variable and let $Y = g(X)$ be a function of this random variable.\n\n1. If $X$ is a discrete random variable, then\n$$\n\\mathbb{E}[g(X)] = \\sum_{x}g(x)f(x)\n$$\n2. If $X$ is a continuous random variable, then\n$$\n\\mathbb{E}[g(X)] = \\int_{x}g(x)f(x)dx\n$$\nNote that $\\mathbb{E}[g(X) + h(X)] = \\mathbb{E}[g(X)] + \\mathbb{E}[h(X)]$.\n\n$\\textbf{Definition 5:}$ The expected value or expectation of a discrete random variable is denoted as $\\mu = \\mathbb{E}(X)$ and is calculated by summing the product of each possible value $x$ of the variable and its corresponding probability $f(x)$:\n$$\n\\mu = \\mathbb{E}(X) = \\sum_{x}xf(x),\n$$\nFor continuous random variables, the expected value $\\mu = \\mathbb{E}(X)$ is obtained through integration. It involves multiplying each value $x$ of the variable by its probability density function $f(x)$ and integrating over the entire range of possible values:\n\n$$\n\\mu = \\mathbb{E}(X) = \\int_{x}xf(x)dx.\n$$\n\n$\\textbf{Definition 6:}$ The variance of a random variable measures the average squared deviation from its expected value. For a discrete random variable $X$, the variance is defined as:\n\n$$\n\\operatorname{var}(X) = \\mathbb{E}\\left[(X-\\mu)^2\\right] = \\sum_{x}\\left[x-\\mathbb{E}(X)\\right]^2f(x),\n$$\n\nSimilarly, for a continuous random variable $X$, the variance is defined as:\n\n$$\n\\operatorname{var}(X) = \\mathbb{E}\\left[(X-\\mu)^2\\right] = \\int_{x}\\left[(X-\\mathbb{E}(X)\\right]^2f(x)dx,\n$$\nWe often use the following way to calculate the variance.\n\n$\\textbf{Property 1:}$\n\n$$\n\\operatorname{var}(X) = \\mathbb{E}\\{[X - \\mathbb{E}(X)]^2\\} = \\mathbb{E}(X^2) - [\\mathbb{E}(X)]^2\n$$\n$\\textit{proof }$\n\n$$\n\\begin{aligned}\n\\operatorname{var}(X) &= \\mathbb{E}[(X - \\mu)^2] = \\mathbb{E}(X^2 + \\mu^2 - 2X\\mu) \\\\\n&= \\mathbb{E}(X^2) - 2\\mu^2 + \\mu^2 = \\mathbb{E}(X^2) - \\mu^2 = \\mathbb{E}(X^2) - [\\mathbb{E}(X)]^2 \n\\end{aligned}\n$$\n\nNotice that the expectation or variance may not exist. For example (Linton, 2017), let $X$ be a random variable whose PMF is defined as\n$$\nf(x) = \n\\begin{cases}\n  \\frac{1}{2|x|(|x|+1)}, \\; & \\text{if } x = \\pm 1, \\pm 2, \\pm 3, \\cdots\\\\\n  0, \\; & \\text{otherwise}.\n\\end{cases}\n$$\nThen we have\n$$\n\\sum_{x=-1}^{-\\infty}\\frac{1}{2|x|(|x|+1)} = -\\infty \\;\\text{ and } \\; \\sum_{x=1}^{\\infty}\\frac{1}{2|x|(|x|+1)} = \\infty,\n$$\nhence $\\mathbb{E}(X)$ does not exist. In terms of the existence of variance, let's consider a random variable $X$ defined as follows:\n$$\nX = \\begin{cases}\n1, & p= \\frac{1}{2}, \\\\\n-1, & p= \\frac{1}{2}.\n\\end{cases}\n$$\n\nTo calculate the variance, we need to determine the expected value $\\mathbb{E}(X)$. However, in this case, the expected value does not exist because the random variable $X$ does not have a well-defined mean.\n\nThe variance is given by $\\operatorname{var}(X) = \\mathbb{E}[(X-\\mu)^2]$, where $\\mu = \\mathbb{E}(X)$. Since $\\mathbb{E}(X)$ is undefined for this random variable, the variance does not exist.\n\nLet's discuss the properties of the expectations and variances.\n\n$\\textbf{Theorem 2:}$ Let $c$ be a constant, then \n\n1. $\\mathbb{E}(c) = c$\n2. $\\mathbb{E}[\\mathbb{E}(X)] = \\mathbb{E}(X)$\n3. $\\operatorname{var}(c) = 0$\n4. $\\mathbb{E}[X - \\mathbb{E}(X)] = \\mathbb{E}(X) - \\mathbb{E}[\\mathbb{E}(X)] = 0$\n5. $\\mathbb{E}(aX + b) = a\\mathbb{E}(X) + b$\n6. $\\operatorname{var}(aX + b) = a^x \\operatorname{var}(X)$\n7. $\\arg\\underset{c}{\\min}\\mathbb{E}[(X - c)^2] \\Rightarrow c = \\mathbb{E}(X)$\n\n$\\textit{proof }$ \n\nFor simplicity, in the following proof will we use discrete random case to show.\n\n1. \n$$\n\\mathbb{E}(c) = \\sum_{x}cf(x) = c\\underbrace{\\sum_{x}f(x)}_{=1} = c.\n$$\n\n2. \nSince $\\mathbb{E}(X)$ is a constant, using the proof shown above, we have\n$$\n\\mathbb{E}[\\mathbb{E}(X)] = \\mathbb{E}(X)\n$$\n\n3. \n$$\n\\operatorname{var}(c) = \\mathbb{E}\\{[c - \\mathbb{E}(c)]^2\\} = \\mathbb{E}(0) = 0\n$$\n4. \n$$\n\\mathbb{E}[X - \\mathbb{E}(X)] = \\mathbb{E}(X - \\mu) = \\mathbb{E}(X) - \\mathbb{E}(\\mu) = \\mathbb{E}(X) - \\mu = 0,\n$$\nwhere $\\mu = \\mathbb{E}(X)$.\n\n5.\n$$\n\\begin{aligned}\n\\mathbb{E}(aX + b) &= \\sum_{x}(ax+b)f(x) = \\sum_{x}af(x) + \\sum_{x} b f(x)\\\\\n&= a \\underbrace{\\sum_{x}xf(x)}_{=\\mu} + b \\underbrace{\\sum_{x}f(x)}_{=1}= a\\mathbb{E}(X) + b\n\\end{aligned}\n$$\nIndeed, we can see $aX + b$ as a function of $X$, and use $\\textbf{Theorem 1}$ to prove.\n\n6.\n$$\n\\begin{aligned}\n\\operatorname{var}(aX + b) &= \\mathbb{E}\\{[aX + b - \\mathbb{E}(aX + b)]^2\\} = \\mathbb{E}\\{[aX + b - (a\\mu + b)]^2\\}\\\\\n&= \\mathbb{E}\\{[a(X - \\mu)^2]\\} = \\mathbb{E}[a^2(X - \\mu)^2] = a^2 \\underbrace{\\mathbb{E}[(X - \\mu)^2]}_{= \\operatorname{var}(X)} = a^x \\operatorname{var}(X)\n\\end{aligned}\n$$\n\n7. \n$$\n\\begin{aligned}\n\\mathbb{E}[(X - c)^2] &= \\mathbb{E}[(X - \\mu + \\mu - c)^2] \\\\\n&= \\mathbb{E}[(X - \\mu)^2 + (\\mu - c)^2 + 2(X - \\mu)(\\mu - c)]\\\\\n&= \\underbrace{\\mathbb{E}[(X - \\mu)^2]}_{= \\operatorname{var}(X)} + \\underbrace{\\mathbb{E}[(\\mu - c)^2]}_{(\\mu - c)^2} + 2 \\underbrace{\\mathbb{E}[(X - \\mu)(\\mu - c)]}_{= 0}\\\\\n&=\\operatorname{var}(X) + (\\mu - c)^2\n\\end{aligned}\n$$\nNow let $Q = (\\mu - c)^2$ be a target function, by first order condition, we have\n$$\n\\frac{\\partial Q}{\\partial c} = - 2(\\mu - c) \\overset{*}{=}0 \\Rightarrow c = \\mu = \\mathbb{E}(X) 1\n$$\n\n### Moment Generating Function (MGF)\n\nThe moment generating function (MGF) is a powerful tool in probability theory and statistics that provides a way to uniquely characterize the probability distribution of a random variable. The MGF allows us to derive moments of a random variable and provides a convenient method for analyzing its properties.\n\n$\\textbf{Definition 7:}$ If $X$ is a discrete/continuous with pmd/pdf $f(x)$. Given $h > 0$ and $t \\in (-h, h)$, if\n$$\nM_{X}(t) = \\mathbb{E}(e^{tX}) = \n\\begin{cases}\n\\sum_{x}e^{tx}f(x) = \\sum_{x}e^{tx}\\mathbb{P}(X = x) \\qquad &\\text{(discrete)}\\\\\n\\int_{x}e^{tx}f(x)dx \\qquad &\\text{(continuous)}\n\\end{cases}\n$$\nand $M_{X}(t)$ exists and is finite, then we say $M_{X}(t)$ a moment generating function. Note that there are two main functions of MGF:\n\n1. Moments: By taking derivatives of the MGF with respect to $t$, we can obtain the moments of the random variable. Specifically, the $n$-th derivative of the MGF evaluated at $t=0$ gives the $n$-th moment of $X$.\n\n2. Uniqueness: If two random variables have the same MGF within a certain range of $t$, they have the same probability distribution. This property allows us to identify and compare probability distributions based on their MGFs.\n\n$\\textbf{Property 2:}$\n\n$$\n\\mathbb{E}(X^{k}) = M_{X}^{(k)}(t)\\Big|_{t = 0}\n$$\n$\\textit{proof }$ \n\nTo prove the relationship $\\mathbb{E}(X^{k}) = M_{X}^{(k)}(t)\\Big|_{t = 0}$, we need to use the Taylor series expansion of the moment-generating function (MGF) $M_{X}(t)$.\n\nSince the Maclaurin series of $e^{y}$ at $y = 0$ is\n$$\ne^{y} = 1 + y + \\frac{y^2}{2!} + \\frac{y^3}{3!} + \\cdots\n$$\nLet $y = tX$, we have\n$$\ne^{tX} = 1 + tX + \\frac{(tX)^2}{2!} + \\frac{(tX)^3}{3!} + \\cdots\n$$\nTake expected value of both sides, we obtain\n$$\n\\mathbb{E}(e^{tX}) = \\Big[1 + tX + \\frac{(tX)^2}{2!} + \\frac{(tX)^3}{3!} + \\cdots\\Big] = 1 + t\\mathbb{E}(X)+ \\frac{t^2}{2!}\\mathbb{E}(X^2) + \\frac{t^3}{3!}\\mathbb{E}(X^3) + \\cdots\n$$\nBy taking the $k$-th derivative with respect to $t$ on both sides of the equation, we obtain\n$$\nM_{X}^{(k)}(t) = \\mathbb{E}(X^{k}) + t\\mathbb{E}(X^{k+1}) + \\frac{t^2}{2!}\\mathbb{E}(X^{k+2}) + \\cdots\n$$\nWe substitute $t = 0$ into the equation we get $\\mathbb{E}(X^{k})$.\n\n$\\textbf{Theorem 3:}$ Given the MGF of $X$ is $M_{X}(t)$. Suppose $Y = aX + b$, then\n$$\nM_{Y}(t) = M_{aX + b}(t) = e^{bt}M_{X}(at)\n$$\n$\\textit{proof }$ \n$$\n\\begin{aligned}\nM_{Y}(t) &= \\mathbb{E}(e^{tY}) = \\mathbb{E}[e^{t(aX + b)}]\\\\\n&= \\mathbb{E}(e^{bt}e^{atX}) = e^{bt}\\mathbb{E}[e^{(at)X}] = e^{bt}M_{X}(at)\n\\end{aligned}\n$$\n$\\textbf{Theorem 4:}$ Suppose that $X_{1},\\cdots, X_{n}$ are $n$ independent random variables; and for $i = 1, \\cdots , n$, let $\\psi_{i}$ denote the MGF of $X_{i}$. Let $Y = X_{1} + \\cdots + X_{n}$, and let the MGF of $Y$ be denoted by $\\psi$. Then for every value of $t$ such that $\\psi_{i}(t)$ is finite for $i = 1, \\cdots , n$,\n$$\n\\psi(t) = \\prod^{n}_{i=1}\\psi_{i}(t).\n$$\n$\\textit{proof }$\nBy definition,\n$$\n\\psi(t) = \\mathbb{E}(e^{tY}) = \\mathbb{E}[e^{t(X_{1} + \\cdots + X_{n})}] = \\mathbb{E}\\left(\\prod^{n}_{i=1}e^{tX_{i}}\\right)\n$$\nSince the random variables $X_{1},\\cdots, X_{n}$ are independent,\n$$\n\\mathbb{E}\\left(\\prod^{n}_{i=1}e^{tX_{i}}\\right) = \\prod^{n}_{i=1}\\mathbb{E}(e^{tX_{i}})\n$$\nHence,\n$$\n\\psi(t) = \\prod^{n}_{i=1}\\psi_{i}(t).\n$$\n## Transformation of Random Variables\n\nFirst, we will discuss discrete random variable.\n\n$\\textbf{Property 3:}$ Given a discrete random variable $X$ with PMF $f(x)$. If $Y = g(X)$, then\n$$\nf_{Y}(y) = \\mathbb{P}(Y = y) = \\mathbb{P}[g(X) = y] = \\sum_{\\{x|g(x) = y\\}}f_{X}(x)\n$$\nFor example, suppose a random variable with PMF:\n$$\nf(x) = \n\\begin{cases}\n\\frac{1}{2}, \\;&x = 0\\\\\n\\frac{1}{4}, \\;&x = 1\\\\\n\\frac{1}{8}, \\;&x = 2\\\\\n\\frac{1}{8}, \\;&x = 3\\\\\n0,\\;&\\text{otherwise.}\n\\end{cases}\n$$\nLet $Y = (X+1)^2$. Then the PMF of $Y$ is\n$$\nf(y) = \n\\begin{cases}\n\\frac{1}{2}, \\;&y = 1\\\\\n\\frac{1}{4}, \\;&y = 4\\\\\n\\frac{1}{8}, \\;&y = 9\\\\\n\\frac{1}{8}, \\;&y = 16\\\\\n0,\\;&\\text{otherwise.}\n\\end{cases}\n$$\nTherefore,\n$$\n\\begin{aligned}\n\\mathbb{E}(Y) &= \\sum_{Y}y f(y) = \\frac{37}{8}\\\\\n\\mathbb{E}(Y^2) &= \\sum_{Y}y^2 f(y) = \\frac{373}{8}\\\\\n\\operatorname{var}(Y) &= \\mathbb{E}(Y^2) - [\\mathbb{E}(Y)]^2 = \\frac{1615}{64}\n\\end{aligned}\n$$\n\nNow we will discuss continuous random variable. \n\n$\\textbf{Property 4:}$ If the PDF of $X$ is $f_{X}(x)$, and $Y = g(X)$ is an injective function under $\\operatorname{supp}(X)$, i.e., $g'(x) \\neq, \\forall x$, then $X = g^{-1}(Y)$. Let the PDF of $Y$ be $f_{Y}(y)$, then\n$$\nf_{Y}(y) = f_{X}[g^{-1}(y)]\\Bigg|\\frac{\\partial g^{-1}(y)}{\\partial y}\\Bigg|,\n$$\n\nwhere $\\Bigg|\\frac{\\partial g^{-1}(y)}{\\partial y}\\Bigg|$ can be abbreviated as $|J|$, called the Jacobian.\n\n$\\textit{proof }$\n\nIn the following proof, we will discuss by two cases.\n\n1. Suppose $g(\\cdot)$ is a strict increasing function. Let $w(x) = g^{-1}(x)$, then $g'(x) > 0$ and $w'(x) > 0$. Also $g(x) \\leq y$ must imply $x \\leq g^{-1}(y)$.\n\nBy the definition of CDF, we have\n$$\nF_{Y}(y) = \\mathbb{P}(Y \\leq y) = \\mathbb{P}[g(X) \\leq y] = \\mathbb{P}[X \\leq g^{-1}(y)] = F_{X}[g^{-1}(y)]\n$$\nTake the derivative of the CDF, then we can obatin the PDF:\n$$\n\\begin{aligned}\nf_{Y}(y) &= \\frac{\\partial F_{Y}(y)}{\\partial y} = F'_{X}[g^{-1}(y)]\\frac{\\partial g^{-1}(y)}{\\partial y}\\\\\n&= f_{X}[g^{-1}(y)] \\times \\bigg[\\frac{\\partial g^{-1}(y)}{\\partial y}\\bigg] = f_{X}[g^{-1}(y)] \\times \\Bigg|\\frac{\\partial g^{-1}(y)}{\\partial y}\\Bigg|\n\\end{aligned}\n$$\n2. Suppose $g(\\cdot)$ is a strict decreasing function. Let $w(x) = g^{-1}(x)$, then $g'(x) < 0$ and $w'(x) < 0$. Also $g(x) \\leq y$ must imply $x \\geq g^{-1}(y)$.\n\nBy the definition of CDF, we have\n$$\nF_{Y}(y) = \\mathbb{P}(Y \\leq y) = \\mathbb{P}[g(X) \\leq y] = \\mathbb{P}[X \\geq g^{-1}(y)] = 1 - F_{X}[g^{-1}(y)]\n$$\nTake the derivative of the CDF, then we can obatin the PDF:\n$$\n\\begin{aligned}\nf_{Y}(y) &= \\frac{\\partial F_{Y}(y)}{\\partial y} = -F'_{X}[g^{-1}(y)]\\frac{\\partial g^{-1}(y)}{\\partial y}\\\\\n&= f_{X}[g^{-1}(y)] \\times \\bigg[-\\frac{\\partial g^{-1}(y)}{\\partial y}\\bigg] = f_{X}[g^{-1}(y)] \\times \\Bigg|\\frac{\\partial g^{-1}(y)}{\\partial y}\\Bigg|\n\\end{aligned}\n$$\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}