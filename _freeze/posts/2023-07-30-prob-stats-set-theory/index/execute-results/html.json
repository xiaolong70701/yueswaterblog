{
  "hash": "c5dc1dee96e8197459b98d77e654afb6",
  "result": {
    "markdown": "---\ntitle: \"統計學與機率論\"\nimage: stat preview.png\nlang: en\ndescription: |\n    從機率論的角度刻畫現代統計學的研究方法。\ndate: \"2023-07-30\"\ndraft: FALSE\ncitation:\n  url: https://yueswater-blog.netlify.app/posts/2023-07-29-prob-stats-set-theory/\ncategories: [stats]\n---\n\n\n現代統計學(statistics)與計量經濟學(econometrics)係構建在兩個公理之上。第一個公理認為整個經濟體系可以被視為一個由機率法則(probability law)所掌控的隨機實驗(random experiment)，一般來說我們稱這個隨機過程為資料產生過程(data generating process)。第二個則是主張任何經濟現象(economic outcome)可以被視為是上述隨機實驗的結果。\n\n$\\textbf{Definition 1.1}:$ A *random experiment* is a mechanism which has at least two possible outcomes.\n\n我們在進行實驗後，只會有一個結果出現(one and only one outcome will occur)，但是在執行實驗前，我們對於出現的結果（通常稱為出象）的數量與內容一無所知。這邊必須強調，所謂實驗不一定是真的要如自然科學學科一般，需要拿滴管產生化學反應、拿彈簧測量彈性，此處的實驗泛指觀測與測量。例如在西門町的某間服飾店前站了一整天觀察來顧客年齡，這是一種實驗。又或者是在路上發放問卷調查公司新產品的口味，這也是一種實驗。\n\n一個隨機實驗包含兩個重要的元素：\n\n- 樣本空間(sample space)：即所有結果的集合(set)，記成 $S$、$\\Omega$ 或 $\\operatorname{supp}(\\cdot)$\n\n- 機率(probability)：所有結果出現的概率為何\n\n## 集合論基礎\n\n首先我們給予樣本空間一個定義：\n\n$\\textbf{Definition 1.2}:$ The possible outcomes of a random experiment are called \"basic outcomes\", and the set of all basic outcomes constitutes the *sample space*, which is denoted by $S$.\n\n例如給定一個公正的骰子，其出現結果有 $1$ 至 $6$，則這顆骰子的樣本空間可以寫成\n$$\nS = \\{1, 2, 3, 4, 5, 6\\}\n$$\n\n扔一枚公正硬幣的結果可以寫成樣本空間的形式：\n$$\nS = \\{\\text{Head}, \\text{Tail}\\}\n$$\n\n若今天投擲兩個公正的硬幣，則樣本空間為\n$$\nS = \\{(H, H). (H, T), (T, H), (T,T)\\}\n$$\n\n又或假設今天氣象局測量一地的氣溫，最低溫令為 $t_{0}$，最高溫為 $t_{1}$，則該地氣溫的樣本空間則為\n$$\nS = \\{t \\in \\mathbb{R}: t_{0} \\leq t \\leq t_{1}\\}\n$$\n\n我們可以看出樣本空間可以是可數(countable)或不可數(uncountable)。\n\n值得注意的是，在一個隨機實驗中，我們知道所有可能的出象所形成的集合，但在進行隨機實驗之前，我們不知道也不可能會知道實驗將會出現哪個結果。\n\n$\\textbf{Definition 1.3}:$ An *event* $A$ is a collection of basic outcomes from the sample space $S$ that share certain common features or equivalently obey certain restrictions.\n\n因此一個事件 $A$ 為樣本空間的一個子集(subset)，即\n$$\nA \\subseteq S\n$$\n\n事件中的出象必須滿足一些特點或限制。例如我們擲一個公正的骰子，並令事件 $A$ 為丟出正面的結果，則\n$$\nA = \\{2, 4, 6\\}\n$$\n\n又或我們令事件 $B$ 為質數，則\n$$\nB = \\{2, 3, 5\\}\n$$\n\n在下面的討論中，我們會使用文氏圖(Venn diagram)作為圖例，其可以用來表達樣本空間、事件或相關的概念。\n\n### 交集(Intersection)\n\n所謂交集，代表元素同時出現在兩個或兩個以上的集合內。或者用統計學的語言來說，就是結果同時出現。我們給予交集數學上的定義：\n\n$\\textbf{Definition 1.4}:$ *Intersection* of $A$ and $B$ , denoted $A \\cap B$, is the set of basic outcomes in $S$ that belong to both $A$ and $B$, i.e.,\n$$\nA \\cap B = \\{x: x \\in A \\text{ and } x \\in B\\}\n$$\n\n上面已經提過，我們要求隨機實驗的出象不能同時出現，在機率論上我們稱其為互斥(exclusive)。\n\n$\\textbf{Definition 1.5}:$ If $A$ and $B$ have no common basic outcomes, they are called mutually exclusive and their intersection is empty set $\\varnothing$, i.e., $A \\cap B = \\varnothing$, where $\\varnothing$ denotes an empty set that contains nothing.\n\n互斥事件也稱為不相交(disjoint)，因為在文氏圖中它們不會重疊。任何互斥事件不可能同時發生。舉例來說，樣本空間 $S$ 中的任意兩個基本結果都是互斥事件。\n\n### 聯集(Union)\n\n聯集則是指元素可能出現在某一個集合內，或在其他集合內。數學上對於聯集的定義為：\n\n$\\textbf{Definition 1.6}:$ The *union* of $A$ and $B$, $A \\cup B$, is the set of all basic outcomes in $S$ that belong to either $A$ or $B$, i.e.,\n$$\nA \\cup B = \\{x: x \\in A \\text{ or } x \\in B\\}\n$$\n\n假設 $A_{i} \\subseteq S$，其中 $i = 1, 2, \\cdots, n$，則若\n$$\n\\bigcup^{n}_{i = 1}A_{i} = S\n$$\n我們稱這 $n$個事件為完全窮盡(collective exhaustiveness)。\n\n### 集合的關係\n\n接下來我們要討論兩個重要的集合關係。首先是補集(complement)，\n\n$\\textbf{Definition 1.7}:$ The *complement* of $A$ is the set of basic outcomes of a random experiment belonging to $S$ but not to $A$, denoted as $A^{c}$ or $A^{\\prime}$, i.e.,\n$$\nA^{c} = \\{x \\notin A: x \\in S\\}\n$$\n\n\n而補集有以下的性質：任何事件 $A$ 及其補集 $A^{c}$，兩者是互斥且完全窮舉的。也就是說：\n\n- 互斥：$A \\cap A^{c} = \\varnothing$\n\n- 完全窮舉：$A \\cup A^{c} = S$\n\n另一個關係是差集(difference)，即是所有屬於 $A$ 但不屬於 $B$ 的元素組成的集合。\n\n$\\textbf{Definition 1.8}:$ The difference of A and B , denoted as $A -B = A \\cap B^{c}$, is the set of basic outcomes in $S$ that belong to $A$ but not to $B$, i.e.,\n$$\nA - B = \\{x \\in A: x \\notin B\\}\n$$\n\n### 集合的運算\n\n假設我們給定三個集合 $A, B, C \\subseteq S$，則我們有以下補集關係：\n$$\n(A^{c})^{c} = A,\\; \\varnothing^{c} = S,\\; S^{c} = \\varnothing\n$$\n\n接著來討論一些集合的運算。\n\n- 交換律(commutative property)：\n$$\n\\begin{aligned}\nA \\cup B &= B \\cup A\\\\\nA \\cap B &= B \\cap A\n\\end{aligned}\n$$\n\n- 結合律(associative property)：\n$$\n\\begin{aligned}\n(A \\cup B) \\cup C &= A \\cup (B \\cup C)\\\\ \n(A \\cap B) \\cap C &= A \\cap (B \\cap C)\n\\end{aligned}\n$$\n\n- 分配律(distributive property)：\n$$\n\\begin{aligned}\nA \\cap (B \\cup C) &= (A \\cap B) \\cup (A \\cap C)\\\\\nA \\cup (B \\cap C) &= (A \\cup B) \\cap (A \\cup C)\n\\end{aligned}\n$$\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;若 $n \\geq 1$，則\n\n$$\n\\begin{aligned}\nB \\bigcap \\left(\\bigcup^{n}_{i = 1}A_{i}\\right) &= \\bigcup^{n}_{i = 1}\\left(B \\bigcap A_{i}\\right)\\\\\nB \\bigcup \\left(\\bigcap^{n}_{i = 1}A_{i}\\right) &= \\bigcap^{n}_{i = 1}\\left(B \\bigcup A_{i}\\right)\\\\\n\\end{aligned}\n$$\n\n- De Morgan 法則：\n\n$$\n\\begin{aligned}\n(A \\cup B)^{c} &= A^{c} \\cap B^{c}\\\\\n(A \\cap B)^{c} &= A^{c} \\cup B^{c}\n\\end{aligned}\n$$\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;若 $n \\geq 1$，則\n\n$$\n\\begin{aligned}\n\\left(\\bigcup^{n}_{i = 1}A_{i}\\right)^{c} = \\bigcap^{n}_{i = 1}A_{i}^{c}\\\\\n\\left(\\bigcap^{n}_{i = 1}A_{i}\\right)^{c} = \\bigcup^{n}_{i = 1}A_{i}^{c}\n\\end{aligned}\n$$\n\n## 機率論基礎\n\n上面我們已經定義了何謂事件，接下來我們為了賦予每個事件機率，我們要討論何謂機率函數(probability function)。\n\n### 空間與機率函數\n\n機率函數可以簡單理解為：一個能夠將事件、事件的補集、事件的交集、聯集映射至(mapping to)實數的函數，因此這個函數的自變量為事件與事件的延伸之組合，這樣的事件集合被稱為樣本空間 $S$ 的一個 $\\sigma$-field 子集合，它將構成機率函數的定義域。\n\n$\\textbf{Definition 1.9}:$ A $\\sigma$-algebra, denoted by $\\mathcal{F}$, is a collection of\nsubsets(events) of $S$ that satisfies the following properties:\n\n- $X\\in {\\mathcal{F}}$ and $\\varnothing \\in \\mathcal{F}$;\n\n- $A\\in {\\mathcal {F}}\\;\\Rightarrow \\;A^{c}\\in {\\mathcal {F}}$;\n\n- $A_{n}\\in {\\mathcal {F}},\\;\\forall n\\in \\mathbb {N} \\;\\Rightarrow \\;\\bigcup_{n=1}^{\\infty }A_{n}\\in {\\mathcal {F}}$\n\n注意到 $\\sigma$-field 是 $S$ 中子集合的集合，但是 $\\sigma$-field 本身並非 $S$ 的子集，而 $S$ 則是 $\\sigma$-field 唯一的元素。我們稱 $(S, \\mathcal{F})$ 為一個可測空間(measurable space)。\n\n$\\textbf{Example 1.1}:$ Show that for any sample space $S$ , then set $\\mathcal{B} = \\{\\varnothing, S\\}$ is always a $\\sigma$-field.\n\n$\\textit{Sol.}$ \n\n<p></p>\n\nWe verify the three properties of a $\\sigma$-field:\n\n- $\\varnothing \\in \\{\\varnothing, S\\}$. Thus $\\varnothing \\in \\mathcal{B}$.\n\n- $\\varnothing^{c} = S \\in \\mathcal{B}$ and $S^{c} = \\varnothing \\in \\mathcal{B}$.\n\n- $\\varnothing \\cup S = S \\in \\mathcal{B}$.\n\n<p align='right'>$\\square$</p>\n\n\n$\\textbf{Example 1.2}:$ Suppose the sample space $S = \\{1,2,3\\}$. Show that a set containing the following eight subsets $\\{1\\}, \\{2\\}, \\{3\\}, \\{1,2\\}, \\{1,3\\}, \\{2,3\\}, \\{1,2,3\\}$, and $\\varnothing$ is a $\\sigma$-field.\n\n$\\textit{Sol.}$ \n\n<p></p>\n\nTrivial.\n\n在引進可測空間的概念後，便可以定義何謂機率函數。\n\n$\\textbf{Definition 1.10}:$ Suppose a random experiment has a sample space $S$ and an associated $\\sigma$-field $\\mathcal{F}$. A probability function\n$$\n\\mathbb{P}: \\mathcal{F} \\to [0, 1]\n$$\n\nis defined as a mapping that satisfies the following three properties:\n\n1. $0 \\leq \\mathbb{P}(A) \\leq 1$ for any event $A$ in $\\mathcal{F}$，此即描述「任何事件都可能出現」。\n\n2. $\\mathbb{P}(S) = 1$，表示每當進行一次隨機實驗時，某一事件總是會發生。\n\n3. If $A_{n} \\in \\mathcal{F}$ are mutually exclusive, then\n$$\n\\mathbb{P}\\left(\\bigcup^{\\infty}_{i = 1}A_{i}\\right) = \\sum^{\\infty}_{i = 1}\\mathbb{P}(A_{i})\n$$\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;這個性質表示獨立事件之和（即聯集）的機率等於它們各自機率的總和。\n\n當我們談論概率函數時，實際上是在描述事件集合 $\\mathcal{B}$ 中事件發生的機率分佈：機率函數告訴我們各個事件的發生機率是如何分布的。\n\n因此，對於一個給定的可測空間 $(S, \\mathcal{F})$，可以定義許多不同的機率函數。計量經濟學和統計學的目標就是找到一個能夠最準確描述資料產生機制的機率函數，這個機率函數通常被稱為真實機率函數或真實機率分布模型。\n\n那麼，要如何去解釋我們對於每個事件都指派的「機率」呢？以下是最常見的兩種解釋方法：\n\n- 相對頻率(relative frequency)：一個事件的機率可以被看作是在大量獨立重複實驗後，在相同的條件下，該事件發生的「相對頻率」的極限。舉例來說，假設我們投擲硬幣總共 $N$ 次。每次投擲時，要麼出現「正面」，要麼出現「反面」。假設在這 $N$ 次實驗中，正面出現了 $N_{h}$次。那麼在這 $N$ 次實驗中，「正面」出現的比例是 $N_{h}/N$。當 $N$ 趨近於無窮大，即 $N \\to \\infty$，該比例 $N_{h}/N$ 的變異將會縮小。\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我們可以用 `R` 來做個實驗。假設我們讓電腦投擲一個公正的硬幣，從 $1$ 次開始，一直投擲到 $1000$ 次，並且紀錄每次投擲後出現正面的相對頻率，來驗證上述的論述是否正確。\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\n\n# 定義模擬硬幣投擲的函數\nsimulate_coin_toss <- function(N) {\n  tosses <- sample(c(\"Head\", \"Tail\"), N, replace = TRUE)\n  num_heads <- sum(tosses == \"Head\")\n  relative_frequency <- num_heads / N\n  return(relative_frequency)\n}\n\n# 執行模擬\nnum_tosses <- 1000  # 指定投擲硬幣的次數N\nrelative_frequencies <- numeric(num_tosses)\n\nfor (i in 1:num_tosses) {\n  relative_frequencies[i] <- simulate_coin_toss(i)\n}\n\n# 繪製相對頻率的折線圖\ndata <- data.frame(N = 1:num_tosses, Relative_Frequency = relative_frequencies)\n\nggplot(data, aes(x = N, y = Relative_Frequency)) +\n  geom_line(color = \"black\") +                      \n  geom_hline(yintercept = 0.5, color = \"#Ce0e4e\") +\n  labs(x = \"Number of tosses\",\n       y = \"Relative frequencies\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;可以看到最終這個公正硬幣出現正面的相對頻率會趨近於 $0.5$。注意到相對頻率解釋了在假設相同條件下進行大量重複實驗時是有效的，而在統計學中，這種假設被稱為**獨立且同分布**(independence and identical distribution, IID)。\n\n- 主觀機率(subjective probability)：即經濟行為者依照其信念(belief)、習慣(habit)等認定某一事件的機率為何。例如理性預期(Muth, 1961)假設經濟主體的主觀期望（即在經濟行為者的主觀機率信念下的期望）與數學期望（即在客觀機率分佈下的期望）相一致。[^1]\n\n[^1]:Muth, J. F. (1961, July). [Rational Expectations and the Theory of Price Movements](https://www.jstor.org/stable/1909635). *Econometrica*, 29(3), 315. [https://doi.org/10.2307/1909635](https://doi.org/10.2307/1909635)\n\n$\\textbf{Definition 1.11}:$ A probability space is a triple $(S, \\mathcal{F}, \\mathbb{P})$ where\n\n- $S$ is the sample space corresponding to the outcomes of the underlying random experiment.\n\n- $\\mathcal{F}$ is the $\\sigma$-field of subsets of $S$. These subsets are called events.\n- $P: \\mathcal{F} \\to [0,1]$ is a probability measure.\n\n因此 $(S, \\mathcal{F}, \\mathbb{P})$ 就猶如三位一體的概念，可以完全地描述在樣本空間 $S$ 上定義的隨機實驗。\n\n$\\textbf{Theorem 1.1}:$ If $\\varnothing$ denotes the empty set, then $\\mathbb{P}(\\varnothing) = 0$.\n\n$\\textit{Proof.}$\n\n<p></p>\n\nGiven that $S = S \\cup \\varnothing$, and $S$ and $\\varnothing$ are mutually exclusive, we then have\n$$\n\\mathbb{P}(S) = \\mathbb{P}(S \\cup \\varnothing) = \\mathbb{P}(S) + \\mathbb{P}(\\varnothing)\n$$\nIt follows that $\\mathbb{P}(\\varnothing) = 0$\n\n<p align='right'>$\\square$</p>\n\n$\\textbf{Theorem 1.2}:$ $\\mathbb{P}(A) = 1 - \\mathbb{P}(A^{c})$\n\n$\\textit{Proof.}$\n\n<p></p>\n\nSince $S = A \\cup A^{c}$, then\n$$\n\\mathbb{P}(S) = \\mathbb{P}(A \\cup A^{c})\n$$\nBy definition, we have $\\mathbb{P}(S) = 1$, and note that $A$ and $A^{c}$ are mutually exclusive, then\n$$\n1 = \\mathbb{P}(A) + \\mathbb{P}(A^{c})\n$$\n\n<p align='right'>$\\square$</p>\n\n$\\textbf{Theorem 1.3}:$ If $A$ and $B$ are two events in $\\mathbb{B}$, and $A \\subseteq B$, then $\\mathbb{P}(A) \\leq \\mathbb{P}(B)$.\n\n$\\textit{Proof.}$\n\n<p></p>\n\nUsing the fact that $S = A \\cup A^{c}$ and the distributive property of sets, we have\n$$\n\\begin{aligned}\nB &= S \\cap B = (A \\cup A^{c}) \\cap B\\\\\n&= (A \\cap B) \\cup (A^{c} \\cap B)\\\\\n&= A \\cup (A^{c} \\cap B)\n\\end{aligned}\n$$\nwhere the last equality follows from $A \\subseteq B$ so that $A \\cap B = A$. Because $A$ and $A^{c} \\cap B$ are mutually exclusive, we have\n\n$$\n\\mathbb{P}(B) = \\mathbb{P}(A) + \\mathbb{P}(A^{c} \\cap B) \\geq \\mathbb{P}(A)\n$$\ngiven that $\\mathbb{P}(A^{c} \\cap B) \\geq 0$.\n\n<p align='right'>$\\square$</p>\n\n而底下是一個最常用的性質：\n\n$\\textbf{Theorem 1.4}:$ For any two events $A$ and $B$ in $\\mathcal{F}$, then\n$$\n\\mathbb{P}(A \\cup B) = \\mathbb{P}(A) + \\mathbb{P}(B) - \\mathbb{P}(A \\cap B)  \n$$\n\n$\\textit{Proof.}$\n\n<p></p>\n\nSince $A \\cup B = A \\cup (A^{c} \\cap B)$ and $A$ and $A^{c} \\cap B$ are mutually exclusive, we have\n$$\n\\mathbb{P}(A \\cup B) = \\mathbb{P}(A) + \\mathbb{P}(A^{c} \\cap B)\n$$\n\nAlso, because $B = S \\cap B = (A \\cap B) \\cup (A^{c} \\cap B)$, and $(A \\cap B)$ and $(A^{c} \\cap B)$ are mutually exclusive, we have\n$$\n\\mathbb{P}(B) = \\mathbb{P}(A \\cap B) + \\mathbb{P}(A^{c} \\cap B)\n$$\n\nBy rearranging both equations, we have\n$$\n\\begin{aligned}\n\\mathbb{P}(A) &= \\mathbb{P}(A \\cup B) - \\mathbb{P}(A^{c} \\cap B)\\\\\n\\mathbb{P}(B) &= \\mathbb{P}(A \\cap B) + \\mathbb{P}(A^{c} \\cap B)\n\\end{aligned}\n$$\n\nAdding both equations and rearraning it gives us the desired result.\n\n<p align='right'>$\\square$</p>\n\n接下來我們要介紹總機率法則(rule of total probability)。\n\n$\\textbf{Theorem 1.5}:$ If $A_{1}, A_{2}, \\cdots in \\mathcal{F}$ are mutually exclusive and collectively exhaustive, and $A$ is an event in $S$, then\n$$\n\\mathbb{P}(A) = \\sum^{\\infty}_{i = 1}\\mathbb{P}(A \\cap A_{i})\n$$\n$\\textit{Proof.}$\n\n<p></p>\n\nNoting\n$$\nS = \\bigcup_{i=1}^\\infty A_i\n$$\nand\n$$\nA = A \\cap S = A \\cap \\bigcup_{i=1}^\\infty A_i = \\bigcup_{i=1}^\\infty (A \\cap A_i)\n$$\nwhere the last equality follows by the distributive law. The result follows because $A \\cap A_i$ and $A \\cap A_j$ are disjoint for all $i \\neq j$.\n\n$\\textbf{Theorem 1.6}:$ For any sequence of events $\\{A_{i} \\in \\mathcal{F}, \\forall i = 1,2, \\cdots\\}$, we have\n\n$$\n{\\mathbb {P} }\\left(\\bigcup _{i=1}^{\\infty }A_{i}\\right)\\leq \\sum _{i=1}^{\\infty }{\\mathbb {P} }(A_{i})\n$$\n\n$\\textit{Proof.}$\n\n<p></p>\n\nLet $B = \\bigcup^{\\infty}_{i = 2}A_{i}$. Then\n$$\n\\bigcup^{\\infty}_{i = 1}A_{i} = A_{1} \\cup B\n$$\nBy using Theorem 1.4, it follows\n$$\n\\begin{aligned}\n\\mathbb{P}\\left(\\bigcup^{n}_{i = 1}A_{i}\\right) &= \\mathbb{P}(A_{1} \\cup B)\\\\\n&= \\mathbb{P}(A_{1}) + \\mathbb{P}(B) - \\mathbb{P}(A_{1} \\cap B)\\\\\n&\\leq \\mathbb{P}(A_{1}) + \\mathbb{P}(B)\n\\end{aligned}\n$$\nwhere where the inequality follows given $P(A_1 \\cap B) \\geq 0$. Again, let $C = \\bigcup_{i=3}^{\\infty} A_i$. Then\n$$\n\\mathbb{P}(B) = \\mathbb{P}(A_{2} \\cup C) \\leq \\mathbb{P}(A_{2}) + \\mathbb{P}(C)\n$$\n\nIt follows that\n$$\n\\mathbb{P}\\left(\\bigcup^{n}_{i = 1}A_{i}\\right) \\leq \\mathbb{P}(A_{1}) + \\mathbb{P}(A_{2}) + \\mathbb{P}(C)\n$$\n\nRepeating this process, we have\n$$\n{\\mathbb {P} }\\left(\\bigcup _{i=1}^{\\infty }A_{i}\\right)\\leq \\sum _{i=1}^{\\infty }{\\mathbb {P} }(A_{i})\n$$\n\n### 條件機率\n\n經濟事件乃至於事件上的所有事件通常彼此存在關聯。例如失業率與職缺率可能存在因果關係（此即 Beveridge 曲線的概念）。由於具有關聯性，事件 $B$ 的發生可能會影響或包含有關事件 $A$ 發生的機率資訊。因此，如果我們擁有有關事件 $B$ 的資訊，我們就能更好地了解事件 $A$ 的發生情況。這可以通過條件機率的概念來描述。\n\n$\\textbf{Definition 1.12}:$ Let $A$ and $B$ be two events in probability space $(S, \\mathcal{F}, \\mathbb{P})$. Then the conditional probability of event $A$ given event $B$, denoted as $\\mathbb{P}(A | B)$, is defined as \n$$\n\\mathbb{P}(A|B) = \\frac{\\mathbb{P}(A \\cap B)}{\\mathbb{P}(B)}\n$$\nprovided that $\\mathbb{P}(B) > 0$. Similarly, the conditional probability of event $B$ given $A$ is defined as\n$$\n\\mathbb{P}(B|A) = \\frac{\\mathbb{P}(A \\cap B)}{\\mathbb{P}(A)}\n$$\nprovided that $\\mathbb{P}(A) > 0$.\n\n在定義條件概率 $\\mathbb{P}(A | B)$ 時，我們假設 $\\mathbb{P}(B)$。這是因為 $\\mathbb{P}(B)$ 意味著事件 $B$ 不太可能發生，而對於一個不太可能發生的事件進行條件機率計算在實際上沒有意義。因此，我們要確保條件機率的分母不為零，也就是要確保事件 $B$ 的機率大於 $0$。\n\n$\\mathbb{P}(A|B)$ 在一個文氏圖中可以表示為事件 $A$ 佔據事件 $B$ 占據的區域相對於事件 $B$ 占據的整個區域。直觀地說，當事件 $B$ 發生時，$B^{c}$ 將永遠不會發生。不確定性從整個樣本空間 $S$ 縮減為事件 $B$，則可測空間變為 $\\left(S\\cap B, \\mathcal{F}\\cap B, \\mathbb{P}(\\cdot|B)\\right)$。因此，當我們考慮 $\\mathbb{P}(A|B)$ 時，我們會將 $B$ 視為一個新的樣本空間，所有後續事件都將根據它們與 $B$ 的關係來進行校正。\n\n因此，我們可以說 $\\mathbb{P}(A|B)$ 描述了如何使用事件 $B$ 的資訊來預測事件 $A$ 的機率。這是 $A$ 和 $B$ 之間的預測關係。但是我們必須注意到，**一個預測關係不一定代表是 $A$ 與 $B$ 的因果關係**，若我們要探討因果關係，則需要仰賴專業理論的相關知識補充說明。\n\n$\\textbf{Lemma 1.1}:$ Given two events $A$ and $B$, then\n\n1. if $\\mathbb{P}(B) > 0$, then $\\mathbb{P}(A \\cap B) = \\mathbb{P}(A|B)\\mathbb{P}(B)$\n\n2. if $\\mathbb{P}(A) > 0$, then $\\mathbb{P}(A \\cap B) = \\mathbb{P}(B|A)\\mathbb{P}(A)$\n\n上面這個關係可以用於計算 $\\mathbb{P}(A \\cap B)$。\n\n$\\textbf{Theorem 1.7}:$ Suppose $\\{A_{i} \\in \\mathcal{F}, \\forall i = 1, \\cdots, n\\}$ is a sequence of $n$ events. Then the joint probability of these $n$ events\n$$\n\\mathbb{P}\\left(\\bigcap^{n}_{i = 1}A_{i}\\right) = \\prod^{n}_{i = 1}\\mathbb{P}\\left(A_{i}|\\bigcap^{i - 1}_{j = 1}A_{j}\\right)\n$$\n\nwith the convention that\n$$\n\\mathbb{P}\\left(A_{1}|\\bigcap^{0}_{j = 1}A_{j}\\right) = \\mathbb{P}(A_{1})\n$$\n\n上述的乘法規則可以被重複使用來獲得多個事件的聯合機率，且注意到對於事件序列 $A_1, A_2, \\ldots, A_n$ 有 $n!$ 不同的條件機率算法。\n\n$\\textbf{Theorem 1.8}:$ Let $\\{A_{i}\\}^{\\infty}_{i = 1}$ be a be a partition (i.e., mutually exclusive and collectively exhaustive) of sample space $S$, with $\\mathbb{P}(A_{i}) > 0$ for $i \\geq 1$. Then for any event $A$ in $\\mathcal{F}$,\n$$\n\\mathbb{P}(A) = \\sum^{\\infty}_{i = 1}\\mathbb{P}(A|A_{i})\\mathbb{P}(A_{i})\n$$\n$\\textit{Proof.}$\n\n<p></p>\n\nBy Theorem 1.5, we have\n$$\n\\mathbb{P}(A) = \\sum^{\\infty}_{i = 1}\\mathbb{P}(A \\cap A_{i})\n$$\n\nThe desired result follows immediately from the multiplication rule that $\\mathbb{P}(A \\cap A_{i}) = \\mathbb{P}(A|A_{i})\\mathbb{P}(A_{i})$.\n\n<p align='right'>$\\square$</p>\n\n### 貝氏定理(Bayes' Theorem)\n\n貝氏定理可謂是條件機率的應用，雖然此定理的起源時間可以追溯到 18 世紀由 Thomas Bayes 提出的概念，但是在 21 世紀的今天，貝氏定理被廣泛的使用在諸多領域上，包含機器學習(machine learning)、政治經濟學(political economy)。\n\n貝氏定理描述的是一個修正或更新先驗概率的機制，當事件 $B$ 發生後，我們可以利用這個信息來修正或更新事件 $A$ 發生的先驗機率(prior probability)。\n\n$\\textbf{Theorem 1.9}:$ Suppose $A$ and $B$ are two events with $\\mathbb{P}(A) > 0$ and $\\mathbb{P}(B) > 0$. Then\n$$\n\\mathbb{P}(A|B) = \\frac{\\mathbb{P}(B|A)\\mathbb{P}(A)}{\\mathbb{P}(B|A)\\mathbb{P}(A) + \\mathbb{P}(B|A^{c})\\mathbb{P}(A^{c})}\n$$\n\n$\\mathbb{P}(A)$ 被稱為先驗概率，即在事實或證據之前，因為它是在新資訊 $B$ 發生之前對事件 $A$ 的機率估計。而條件機率 $\\mathbb{P}(A|B)$ 被稱為後驗概率(posterior probability)，即在事實或證據之後，因為它代表在獲得事件 $B$ 發生的新資訊之後，對事件 $A$ 機率進行修正後的分配。\n\n因此貝氏定理可以用更白話的方式呈現，即在事件 $A$ 發生後，事件 $A$ 的後驗概率與樣本證據 $B$ 的概率成正比，比例常數為事件 $A$ 的先驗概率。故我們可以說貝氏定理表達了主觀信念程度應該如何合理地改變以適應相關證據的可用性。[^2]\n\n[^2]: 不妨可以參考成語「曾參殺人」的典故。\n\n$\\textbf{Theorem 1.10}:$ Suppose $A_{1}, \\cdots, A_{n}$ are $n$ mutually exclusive and collectively exhaustive events in the sample space $S$, and $A$ is an event with $\\mathbb{P}(A) > 0$. Then the conditional probability of $A_{i}$ given $A$ is\n$$\n\\mathbb{P}(A|A_{i}) = \\frac{\\mathbb{P}(A|A_{i})\\mathbb{P}(A_{i})}{\\sum^{n}_{j = 1}\\mathbb{P}(A|A_{j})\\mathbb{P}(A_{j})}, \\forall i = 1, \\cdots, n\n$$\n\n$\\textit{Proof.}$\n\n<p></p>\n\nBy the conditional probability definition and multiplication rule, we have\n$$\n\\begin{aligned}\n\\mathbb{P}(A_{i}|A) &= \\frac{\\mathbb{P}(A_{i} \\cap A)}{\\mathbb{P}(A)}\\\\\n&= \\frac{\\mathbb{P}(A|A_{i})\\mathbb{P}(A_{i})}{\\mathbb{P}(A)}\n\\end{aligned}\n$$\n\nBecause $\\{A_{i}\\}^{n}_{i = 1}$ are collectively exhaustive and mutually exclusive, from the rule of total probability in Theorem 1.8, we have\n$$\n\\begin{aligned}\n\\mathbb{P}(A) &= \\sum^{n}_{j = 1}\\mathbb{P}(A \\cap A_{j})\\\\\n&= \\sum^{n}_{j = 1}\\mathbb{P}(A|A_{j})\\mathbb{P}(A_{j})\n\\end{aligned}\n$$\n\n底下我們來探討一個汽車保險公司保費(auto-insurance premium)的例子。\n\n$\\textbf{Example 1.2}:$ Suppose an insurance company has three types of customers high risk, medium risk and low risk. From the company's historical consumer database, it is known that $25\\%$ of its customers are high risk, $25\\%$ are medium risk, and $50\\%$ are low risk. Also, the database shows that the probability that a customer has at least one speeding ticket in one year is $0.25$ for high risk, $0.16$ for medium risk, and $0.10$ for low risk. \n\nNow suppose a new customer wants to be insured and reports that he has had one speeding ticket this year. What is the probability that he is a high risk customer, given that he has had one speeding ticket this year?\n\n$\\textit{Sol.}$\n\n<p></p>\n\nTo calculate the probability that the new customer is a high-risk customer, given that he has had one speeding ticket this year, we can use Bayes' theorem.\n\nLet $H$ be the event that the customer is high risk, and $T$ be the event that the customer has had one speeding ticket this year.\n\nWe want to find $\\mathbb{P}(H|T)$, the probability that the customer is high risk given that he has had one speeding ticket.\n\nAccording to Bayes' theorem:\n\n$$\n\\mathbb{P}(H|T) = \\frac{\\mathbb{P}(T|H) \\cdot \\mathbb{P}(H)}{\\mathbb{P}(T)}\n$$\n\nwhere\n\n- $\\mathbb{P}(T|H)$ is the probability that the customer has had one speeding ticket given that he is high risk.\n\n- $\\mathbb{P}(H)$ is the probability that a customer is high risk.\n- $\\mathbb{P}(T)$ is the probability that the customer has had one speeding ticket, which can be calculated using the law of total probability:\n\n$$\n\\mathbb{P}(T) = \\mathbb{P}(T|H) \\cdot \\mathbb{P}(H) + \\mathbb{P}(T|M) \\cdot \\mathbb{P}(M) + \\mathbb{P}(T|L) \\cdot \\mathbb{P}(L)\n$$\n\nwhere:\n\n- $\\mathbb{P}(T|H)\\mathbb{P}$ is the probability of one speeding ticket given high risk (0.25).\n\n- $\\mathbb{P}(T|M)$ is the probability of one speeding ticket given medium risk.\n\n- $\\mathbb{P}(T|L)$ is the probability of one speeding ticket given low risk.\n\n- $\\mathbb{P}(M)$ is the probability of a customer being medium risk.\n\n- $\\mathbb{P}(L)$ is the probability of a customer being low risk.\n\nNow, let's plug in the values and calculate:\n\n$$\n\\mathbb{P}(T) = 0.25 \\cdot 0.25 + 0.16 \\cdot 0.25 + 0.10 \\cdot 0.50 = 0.0625 + 0.04 + 0.05 = 0.1525\n$$\n\nFinally, we can calculate $\\mathbb{P}(H|T)$:\n\n$$\n\\mathbb{P}(H|T) = \\frac{0.25 \\cdot 0.25}{0.1525} \\approx 0.4098\n$$\n\nSo, the probability that the new customer is a high-risk customer, given that he has had one speeding ticket this year, is approximately $0.4098$ or $40.98\\%$.\n\n### 獨立事件\n\n獨立事件(independent event)是指兩個或多個事件之間彼此不影響、不相關的情況。換句話說，一個事件的發生或不發生並不會影響其他事件的機率。\n\n$\\textbf{Definition 1.13}:$ Events $A$ and $B$ are said to be statistically independent\nif $\\mathbb{P}(A \\cap B) = \\mathbb{P}(A)\\mathbb{P}(B)$\n\n具有獨立性的事件被稱為在統計學上是獨立、隨機獨立的，或在機率上是獨立的。在大多數情況下，我們使用「獨立」一詞來描述上述的情況。\n\n那我們可以從獨立的性質得出什麼結論呢？首先，假設 $\\mathbb{P}(B) > 0$，則\n$$\n\\begin{aligned}\n\\mathbb{P}(A|B) &= \\frac{\\mathbb{P}(A \\cap B)}{\\mathbb{P}(B)}\\\\\n&= \\frac{\\mathbb{P}(A)\\mathbb{P}(B)}{\\mathbb{P}(B)}\\\\\n&= \\mathbb{P}(A)\n\\end{aligned}\n$$\n\n對於事件 $A$ 和 $B$ 而言，如果 $\\mathbb{P}(B|A)$，表示在已知事件 $B$ 後對事件 $A$ 的預測沒有幫助，即知道事件 $B$ 發生並不改變預測事件 $A$ 發生的機率。直觀上來說，獨立性意味著事件 $A$ 和 $B$是「不相關的(irrelevant)」，或者說它們之間不存在任何關係。\n\n獨立性質為何重要。我們來看一個金融學上的應用：隨機漫步(random walk)。假設有一支股票，其價格令為 $P_{t}$，其中 $t$ 代表時間。該股票價格若滿足以下條件，則稱其符合隨機漫步：\n\n$$\nP_{t} + P_{t-1} = X_{t}\n$$\n\n其中 $\\{X_{t}\\}$ 在不同時間段上是相互獨立的，從上式也可以看出 $X_{t}$ 代表的是價格的增長(increment)，因\n\n$$\nX_{t} = P_{t} - P_{t-1}\n$$\n\n\n![Fig 1.1: 隨機漫步[^3]](randomWalk.gif){fig.align='center' width=\"50%\"}\n\n[^3]: *Brownian motion and random walks*. (n.d.). Brownian Motion and Random Walks. Retrieved July 31, 2023, from [https://web.mit.edu/8.334/www/grades/projects/projects17/OscarMickelin/brownian.html](https://web.mit.edu/8.334/www/grades/projects/projects17/OscarMickelin/brownian.html)\n\n令一個對於隨機漫步的變種定義為，如果股票價格 $P_t$ 符合以下條件，則稱為幾何隨機漫步：\n\n$$\n\\ln P_t = \\ln P_{t-1} + X_t\n$$\n\n根據泰勒展開式(Taylor expansion)，\n$$\nX_t = \\ln\\left(\\frac{P_t}{P_{t-1}}\\right) = \\ln\\left(1 + \\frac{P_t - P_{t-1}}{P_{t-1}}\\right) \\approx \\frac{P_t - P_{t-1}}{P_{t-1}}\n$$\n代表價格變動的比例。幾何隨機漫步假設最重要的含義是：如果在不同的時間段內，$X_{t}$ 是序列獨立的，則未來的股票價格變化 $X_{t}$ 不能透過歷史股價資料來預測。在這樣的情況下，我們稱股票市場具有資訊效率(informationally efficient)。\n\n$\\textbf{Theorem 1.11}:$ Let $A$ and $B$ be two independent events. Then\n\n- $A$ and $B^{c}$ are independent\n\n- $A^{c}$ and $B^{c}$ are independent\n\n- $A^{c}$ and $B^{c}$ are independent\n\n$\\textit{Proof.}$\n\n<p></p>\n\n1. If $\\mathbb{P}\\left(A \\cap B^c\\right)=\\mathbb{P}(A) \\mathbb{P}\\left(B^c\\right)$, then $A$ and $B^c$ are independent. Because $(A \\cap B) \\cup\\left(A \\cap B^{c}\\right)=A$, we have\n$$\n\\mathbb{P}(A \\cap B)+\\mathbb{P}(A \\cap B^{c})=\\mathbb{P}(A) .\n$$\nIt follows from the multiplication rule that\n$$\n\\begin{aligned}\n\\mathbb{P}(A \\cap B^c) & =\\mathbb{P}(A)-\\mathbb{P}(A \\cap B) \\\\\n& =\\mathbb{P}(A)-\\mathbb{P}(A) \\mathbb{P}(B) \\\\\n& =\\mathbb{P}(A)\\left[1-\\mathbb{P}(B)\\right] \\\\\n& =\\mathbb{P}(A) \\mathbb{P}(B^c)\n\\end{aligned}\n$$\n\n2. By symmetry.\n\n3. Because $(A \\cap B^c) \\cup(A^c \\cap B^c)=B^c$, we have\n$$\n\\mathbb{P}(A \\cap B^c)+\\mathbb{P}(A^c \\cap B^c)=\\mathbb{P}(B^c)\n$$\nIt follows that\n$$\n\\begin{aligned}\n\\mathbb{P}(A^{c} \\cap B^{c}) &= \\mathbb{P}(B^{c}) - \\mathbb{P}(A \\cap B^{c})\\\\\n&= \\mathbb{P}(B^{c}) - \\mathbb{P}(A)\\mathbb{P}(B^{c})\\\\\n&= \\mathbb{P}(A^{c})\\mathbb{P}(B^{c})\n\\end{aligned}\n$$\n\n假設 $A$ 和 $B$ 是獨立的，那麼 $A$ 和 $B^c$ 也應該是獨立的，因為如果不是這樣，就可以用 $A$ 來預測 $B^c$ 的機率，然後通過補集機率公式來預測 $B$ 在給定 $A$ 的條件下的機率：\n$$\n\\mathbb{P}(B|A)=1-\\mathbb{P}(B^c|A)\n$$\n\n$\\textbf{Definition 1.14}:$ Suppose $k$ events $A_{1}, \\cdots, A_{k}$ are mutually independent if, for every possible subset $A_{i_{1}}, \\cdots, A_{i_{j}}$ of $j$ of those events, where $j = 1,2, \\cdots, k$, then\n$$\n\\mathbb{P}(A_{i_{1}} \\cap \\cdots \\cap A_{i_{j}}) = \\mathbb{P}(A_{i_{1}}) \\times \\cdots \\times \\mathbb{P}(A_{i_{j}})\n$$\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}